{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic-Revenue Statistical Analysis\n",
    "\n",
    "This notebook analyzes the correlation between internal traffic data and revenue across the top 10/50/100/250 sites.\n",
    "\n",
    "**Key insight:** Traffic values are monthly estimates at each snapshot, not daily visits. We test different snapshot selection methods to find the strongest correlation with revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 250 sites in ranked order (provided by user)\n",
    "TOP_250_SITES = [\n",
    "    'decoratoradvice.com', 'lookwhatmomfound.com', 'famousparenting.com', 'betterthisworld.com',\n",
    "    'thestripesblog.com', 'etruesports.com', 'thenervousbreakdown.com', 'GFXMaker.com',\n",
    "    'activepropertycare.com', 'playmyworld.com', 'bizwebgenius.com', 'redandwhitemagz.com',\n",
    "    'myfavouriteplaces.org', 'voicesofconservation.org', 'mygreenbucks.net', 'beaconsoft.net',\n",
    "    'traveltweaks.com', 'conversationswithgreg.com', 'thegamearchives.com', 'goodnever.com',\n",
    "    'factom.com', 'revolvertech.com', 'hikhanacademy.org', 'lyncconf.com', 'thinkofgames.com',\n",
    "    'goldengatemax.shop', 'aliensync.com', 'arcyart.com', 'mustardseedyear.com', 'disquantified.org',\n",
    "    'harmonicode.com', 'termanchor.com', 'embedtree.com', 'whatutalkingboutwillis.com', 'smartsatta.com',\n",
    "    'thechannelrace.org', 'bookspersonally.com', 'oneframework.net', 'projectrethink.org', 'gatorgross.com',\n",
    "    'livingpristine.com', 'conversationswithbrittany.com', 'geekgadget.net', 'hearthstats.net',\n",
    "    'coventchallenge.com', 'crypticstreet.com', 'freelogopng.com', 'wealthybyte.com',\n",
    "    'amairaskincare.com.au', 'letsbuildup.org', 'doanphuongkimlien.com', 'luxuryinteriors.org',\n",
    "    'whtvbox.com', 'mygardenandpatio.com', 'digitalrgs.org', 'masterrealtysolutions.com',\n",
    "    'banlarbhumi.com', 'feathershort.com', 'fameblogs.net', 'middleclasshomes.net', 'icaiorg.net',\n",
    "    'filmyzillah.com', 'logicalshout.com', 'playmastersclub.net', 'alternativeway.net',\n",
    "    'mucicallydown.com', 'apartamentoscholloapartamentos.com', 'nationalpainreport.com',\n",
    "    'w88w88hanoi.com', 'twiliaorg.com', 'thehake.com', 'setupseeker.xyz', 'playdedeus.com',\n",
    "    'av19org.net', 'majinoukari.com', 'themeshgame.com', 'essec-kpmg.net', 'dhilisatta.com',\n",
    "    'netzgames.net', 'eurogamersonline.com', 'harmoniclast.com', 'thinksano.com', 'jerseyexpress.net',\n",
    "    'greediegoddess.com', 'iboomatelugu.com', 'dm-gaming.com', 'tech-biztech.com', 'besttarahi.com',\n",
    "    'mazinoukari.com', 'etherions.com', 'zerodevice.net', 'thehometrotters.com', 'onthisveryspot.com',\n",
    "    'g15tools.com', 'quantumcontactsshop.com', 'toptenlast.com', 'designmode24.com', 'aegaming.tv',\n",
    "    'myinteriorpalace.com', 'bytesize-games.com', 'protongamer.com', 'bitnation-blog.com', 'dwchr.com',\n",
    "    'onoxservices.com', 'musikcalldown.com', 'gamerawr.com', 'insidetheelevator.com', 'sportsfanfare.com',\n",
    "    'satkamataka.com', 'buydomainspremium.com', 'freewayget.com', 'plugboxlinux.org', 'evolvedgross.com',\n",
    "    'metsuiteorg.com', 'avstarnews.com', 'towersget.com', 'igxocosmetics.com', 'misumiskincare.com',\n",
    "    'pawerbet.com', 'beerandmagic.com', 'fb88eu.net', 'bouncemediagroup.com', 'seismicpostshop.com',\n",
    "    'venky12.com', 'theportablegamer.com', 'wibexeorg.com', 'songoftruth.org', 'reactcheck.com',\n",
    "    'pushyourdesign.com', 'fudbollibre.com', 'elisehurstphotography.com', 'skylightvoice.com',\n",
    "    'sports-report.net', 'conversationswithbianca.com', 'drhomey.com', 'quantumflooingservices.com',\n",
    "    'areatsunami.com', 'cowded.com', 'entretech.org', 'investirebiz.com', 'mywirelesscoupons.com',\n",
    "    'healthsciencesforum.com', 'nothing2hide.net', 'theboringmagazine.com', 'wolfcontactsshop.com',\n",
    "    'wizzydigital.org', 'turbogeek.org', 'passionate-culinary-enterprises.com', 'triumphgross.com',\n",
    "    'tubitymusic.com', 'ck2generator.com', 'checkerpointorg.com', 'northshoretimingonline.com',\n",
    "    'sportsblitzzone.com', 'annoncetravesti.com', 'cookiesforlove.com', 'robthecoins.com',\n",
    "    'tuple-tech.com', 'abithelp.com', 'saharahausa.com', 'homerocketrealty.com', 'val9jamusic.com',\n",
    "    'alignlast.net', 'analysistheme.com', 'stayloosemusic.com', 'lotterygamedevelopers.com',\n",
    "    'artsusshop.com', 'theguardianhib.com', 'greenheal.net', 'propagatenetworks.com',\n",
    "    'thelowdownunder.com', 'bizboostpro.com', 'uptempomag.com', '21strongfoundation.org',\n",
    "    'sunnylast.com', 'zigaero.com', 'android-underground.org', 'conversationswithjessica.com',\n",
    "    'coststatus.com', 'pizzlemusic.com', 'thunderonthegulf.com', 'botbrobiz.com', 'blackrocklast.com',\n",
    "    'epicgamerhq.com', 'prositesite.com', 'sportssavvyspot.com', 'cryptopronetwork.com',\n",
    "    'safetyproductsmfg.com', 'formulagross.com', 'grosseasy.com', 'reality-movement.org',\n",
    "    'feedbuzzard.com', 'leaguechannellife.net', 'Formotorbikes.com', 'theblockchainbrief.com',\n",
    "    'liveamoment.org', 'sweedishlove.com', 'fightingforfutures.org', 'tech-bliss.com',\n",
    "    'norstratiamrestaurant.com', 'digitalnewsalerts.com', 'trendsetti.com', 'mpgproworkstation.com',\n",
    "    'toolmilk.com', 'springhillmedgroup.com', 'messgodess.com', 'leopardtheme.com', 'readlists.com',\n",
    "    'eselmomentocv.com', 'shippingbellabeat.com', 'gtchgth.com', 'mydecine.com', 'aggreg8.net',\n",
    "    'jordantrent.com', 'importantcool.com', 'tierraarea.com', 'crystalcreekland.com', 'alwaysthis.com',\n",
    "    'playbattlesquare.com', 'durostech.com', 'beargryllsgear.org', 'fintechasia.net',\n",
    "    'sportscentrehub.com', 'costofwar.com', 'boujeefitshapewear.com', 'letwomenspeak.com',\n",
    "    'anywherestory.net', 'indianmatkamobi.com', 'technolotal.org', 'kdarchitects.net', 'snapsource.net',\n",
    "    'americanlivewire.com', 'greenflourishhome.com', 'filmjila.com', 'thecurrentonline.net',\n",
    "    'crazeforgamez.com', 'verdantnaturehome.com', 'thewritetrackpodcast.com', 'clearingdelight.com',\n",
    "    'kalyanmatkachart.com', 'lotrizlotriz.com', 'gadgetsguruz.com', 'festivefitnessphilly.com',\n",
    "    'terabytelabs.net', 'queenslot800.com', 'deephacks.org', 'kidsturncentral.com', 'anglospeed.com',\n",
    "    'techsolutionsbiz.com', 'residencerenew.com'\n",
    "]\n",
    "\n",
    "# Create lowercase mapping for matching\n",
    "TOP_250_LOWER = [s.lower() for s in TOP_250_SITES]\n",
    "\n",
    "print(f\"Total sites in ranking: {len(TOP_250_SITES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load revenue data\n",
    "revenue_df = pd.read_csv('revenue-history.csv')\n",
    "print(f\"Revenue data shape: {revenue_df.shape}\")\n",
    "print(f\"Columns: {revenue_df.columns.tolist()[:10]}... (showing first 10)\")\n",
    "revenue_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load traffic data\n",
    "traffic_df = pd.read_csv('traffic-data.csv')\n",
    "print(f\"Traffic data shape: {traffic_df.shape}\")\n",
    "print(f\"Columns (first 10): {traffic_df.columns.tolist()[:10]}\")\n",
    "traffic_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DR history\n",
    "dr_df = pd.read_csv('DR History.csv')\n",
    "print(f\"DR data shape: {dr_df.shape}\")\n",
    "dr_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_revenue(val):\n",
    "    \"\"\"Parse revenue value from string format like '$1,234.56' to float\"\"\"\n",
    "    if pd.isna(val) or val in ['-', 'x', '', ' ']:\n",
    "        return np.nan\n",
    "    try:\n",
    "        # Handle string values\n",
    "        if isinstance(val, str):\n",
    "            val = val.strip().replace('$', '').replace(',', '')\n",
    "            if val in ['-', 'x', '', ' ']:\n",
    "                return np.nan\n",
    "        return float(val)\n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan\n",
    "\n",
    "# Test the parser\n",
    "test_vals = ['$1,234.56', '-', 'x', '$375.00', None, '']\n",
    "print(\"Revenue parser test:\")\n",
    "for v in test_vals:\n",
    "    print(f\"  '{v}' -> {parse_revenue(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process revenue data\n",
    "# First row contains the date info, Website column has domain names\n",
    "# Identify month columns (they contain years like 2022, 2023, etc.)\n",
    "\n",
    "# Get the column that contains website names\n",
    "website_col = 'Website'\n",
    "\n",
    "# Identify month columns (format like 'Jan 2022', 'Feb 2022', etc.)\n",
    "month_cols = [col for col in revenue_df.columns if any(year in str(col) for year in ['2022', '2023', '2024', '2025', '2026'])]\n",
    "print(f\"Found {len(month_cols)} month columns\")\n",
    "print(f\"Sample month columns: {month_cols[:5]}\")\n",
    "\n",
    "# Filter to rows with valid website names\n",
    "revenue_clean = revenue_df[revenue_df[website_col].notna() & (revenue_df[website_col] != '-')].copy()\n",
    "print(f\"\\nRevenue rows with valid websites: {len(revenue_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean revenue dataframe in long format\n",
    "revenue_records = []\n",
    "\n",
    "for idx, row in revenue_clean.iterrows():\n",
    "    website = str(row[website_col]).strip().lower()\n",
    "    niche = row.get('Niche', 'Unknown')\n",
    "    \n",
    "    for month_col in month_cols:\n",
    "        rev_val = parse_revenue(row[month_col])\n",
    "        if not pd.isna(rev_val):\n",
    "            # Parse month column to date\n",
    "            try:\n",
    "                # Handle format like 'Jan 2022'\n",
    "                month_date = pd.to_datetime(month_col, format='%b %Y')\n",
    "                revenue_records.append({\n",
    "                    'website': website,\n",
    "                    'niche': niche,\n",
    "                    'month': month_date,\n",
    "                    'month_str': month_col,\n",
    "                    'revenue': rev_val\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "revenue_long = pd.DataFrame(revenue_records)\n",
    "print(f\"Revenue long format: {len(revenue_long)} records\")\n",
    "print(f\"Unique websites: {revenue_long['website'].nunique()}\")\n",
    "print(f\"Date range: {revenue_long['month'].min()} to {revenue_long['month'].max()}\")\n",
    "revenue_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process traffic data\n",
    "# Traffic data has Website column and date columns like 'Jul 1 - 2024'\n",
    "\n",
    "# Get website column name (might be slightly different)\n",
    "traffic_website_col = traffic_df.columns[1]  # Usually second column after index\n",
    "print(f\"Traffic website column: '{traffic_website_col}'\")\n",
    "\n",
    "# Identify date columns\n",
    "traffic_date_cols = [col for col in traffic_df.columns if '-' in str(col) and any(year in str(col) for year in ['2024', '2025', '2026'])]\n",
    "print(f\"Found {len(traffic_date_cols)} date columns in traffic data\")\n",
    "print(f\"Sample: {traffic_date_cols[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_traffic_date(date_str):\n",
    "    \"\"\"Parse traffic date from format like 'Jul 1 - 2024' to datetime\"\"\"\n",
    "    try:\n",
    "        # Remove the ' - ' and parse\n",
    "        clean = str(date_str).replace(' - ', ' ')\n",
    "        return pd.to_datetime(clean, format='%b %d %Y')\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Test\n",
    "test_dates = ['Jul 1 - 2024', 'Aug 15 - 2025', 'Jan 3 - 2026']\n",
    "for d in test_dates:\n",
    "    print(f\"'{d}' -> {parse_traffic_date(d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create traffic long format\n",
    "traffic_records = []\n",
    "\n",
    "for idx, row in traffic_df.iterrows():\n",
    "    website = str(row[traffic_website_col]).strip().lower() if pd.notna(row[traffic_website_col]) else None\n",
    "    if not website:\n",
    "        continue\n",
    "        \n",
    "    for date_col in traffic_date_cols:\n",
    "        traffic_val = row[date_col]\n",
    "        parsed_date = parse_traffic_date(date_col)\n",
    "        \n",
    "        if parsed_date and pd.notna(traffic_val):\n",
    "            try:\n",
    "                traffic_num = float(traffic_val)\n",
    "                if traffic_num > 0:  # Only include positive values\n",
    "                    traffic_records.append({\n",
    "                        'website': website,\n",
    "                        'date': parsed_date,\n",
    "                        'traffic': traffic_num\n",
    "                    })\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "\n",
    "traffic_long = pd.DataFrame(traffic_records)\n",
    "print(f\"Traffic long format: {len(traffic_long)} records\")\n",
    "print(f\"Unique websites: {traffic_long['website'].nunique()}\")\n",
    "print(f\"Date range: {traffic_long['date'].min()} to {traffic_long['date'].max()}\")\n",
    "traffic_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to top 250 sites\n",
    "revenue_top250 = revenue_long[revenue_long['website'].isin(TOP_250_LOWER)].copy()\n",
    "traffic_top250 = traffic_long[traffic_long['website'].isin(TOP_250_LOWER)].copy()\n",
    "\n",
    "print(f\"Revenue records for top 250: {len(revenue_top250)} ({revenue_top250['website'].nunique()} sites)\")\n",
    "print(f\"Traffic records for top 250: {len(traffic_top250)} ({traffic_top250['website'].nunique()} sites)\")\n",
    "\n",
    "# Check which sites are missing\n",
    "sites_with_revenue = set(revenue_top250['website'].unique())\n",
    "sites_with_traffic = set(traffic_top250['website'].unique())\n",
    "sites_with_both = sites_with_revenue & sites_with_traffic\n",
    "\n",
    "print(f\"\\nSites with both revenue AND traffic data: {len(sites_with_both)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Traffic Snapshot Selection Methods\n",
    "\n",
    "Traffic values are **monthly estimates** at each snapshot. We test 4 methods to align with monthly revenue:\n",
    "1. **Same-month latest** - Last snapshot within the revenue month\n",
    "2. **Same-month average** - Average of snapshots within the month\n",
    "3. **30-day lagged** - Snapshot from ~30 days before month end\n",
    "4. **60-day lagged** - Snapshot from ~60 days before month end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_end(month_date):\n",
    "    \"\"\"Get the last day of the month\"\"\"\n",
    "    next_month = month_date + pd.offsets.MonthEnd(0)\n",
    "    return next_month\n",
    "\n",
    "def get_same_month_latest(traffic_df, website, month_date):\n",
    "    \"\"\"Get the latest traffic snapshot within the same month\"\"\"\n",
    "    month_start = month_date.replace(day=1)\n",
    "    month_end = get_month_end(month_date)\n",
    "    \n",
    "    mask = (traffic_df['website'] == website) & \\\n",
    "           (traffic_df['date'] >= month_start) & \\\n",
    "           (traffic_df['date'] <= month_end)\n",
    "    \n",
    "    subset = traffic_df[mask]\n",
    "    if len(subset) == 0:\n",
    "        return np.nan\n",
    "    return subset.loc[subset['date'].idxmax(), 'traffic']\n",
    "\n",
    "def get_same_month_avg(traffic_df, website, month_date):\n",
    "    \"\"\"Get average of all traffic snapshots within the same month\"\"\"\n",
    "    month_start = month_date.replace(day=1)\n",
    "    month_end = get_month_end(month_date)\n",
    "    \n",
    "    mask = (traffic_df['website'] == website) & \\\n",
    "           (traffic_df['date'] >= month_start) & \\\n",
    "           (traffic_df['date'] <= month_end)\n",
    "    \n",
    "    subset = traffic_df[mask]\n",
    "    if len(subset) == 0:\n",
    "        return np.nan\n",
    "    return subset['traffic'].mean()\n",
    "\n",
    "def get_lagged_traffic(traffic_df, website, month_date, lag_days):\n",
    "    \"\"\"Get traffic snapshot from approximately lag_days before month end\"\"\"\n",
    "    month_end = get_month_end(month_date)\n",
    "    target_date = month_end - pd.Timedelta(days=lag_days)\n",
    "    \n",
    "    # Get all snapshots for this website\n",
    "    website_traffic = traffic_df[traffic_df['website'] == website]\n",
    "    if len(website_traffic) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # Find closest snapshot to target date\n",
    "    website_traffic = website_traffic.copy()\n",
    "    website_traffic['date_diff'] = (website_traffic['date'] - target_date).abs()\n",
    "    closest = website_traffic.loc[website_traffic['date_diff'].idxmin()]\n",
    "    \n",
    "    # Only use if within reasonable range (30 days of target)\n",
    "    if closest['date_diff'].days > 30:\n",
    "        return np.nan\n",
    "    return closest['traffic']\n",
    "\n",
    "# Test the functions\n",
    "test_website = traffic_top250['website'].iloc[0]\n",
    "test_month = pd.Timestamp('2024-10-01')\n",
    "print(f\"Testing with website: {test_website}, month: {test_month}\")\n",
    "print(f\"Same-month latest: {get_same_month_latest(traffic_top250, test_website, test_month)}\")\n",
    "print(f\"Same-month avg: {get_same_month_avg(traffic_top250, test_website, test_month)}\")\n",
    "print(f\"30-day lagged: {get_lagged_traffic(traffic_top250, test_website, test_month, 30)}\")\n",
    "print(f\"60-day lagged: {get_lagged_traffic(traffic_top250, test_website, test_month, 60)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build combined dataset with all 4 traffic methods\n",
    "# Only use months where we have traffic data (Jul 2024 onwards)\n",
    "\n",
    "traffic_min_date = traffic_top250['date'].min()\n",
    "print(f\"Traffic data starts: {traffic_min_date}\")\n",
    "\n",
    "# Filter revenue to months with traffic data available\n",
    "revenue_filtered = revenue_top250[revenue_top250['month'] >= traffic_min_date.replace(day=1)].copy()\n",
    "print(f\"Revenue records in traffic period: {len(revenue_filtered)}\")\n",
    "\n",
    "# Get unique website-month combinations\n",
    "combined_records = []\n",
    "total = len(revenue_filtered)\n",
    "\n",
    "print(\"\\nBuilding combined dataset (this may take a minute)...\")\n",
    "for i, (idx, row) in enumerate(revenue_filtered.iterrows()):\n",
    "    if i % 500 == 0:\n",
    "        print(f\"  Processing {i}/{total}...\")\n",
    "    \n",
    "    website = row['website']\n",
    "    month = row['month']\n",
    "    \n",
    "    record = {\n",
    "        'website': website,\n",
    "        'month': month,\n",
    "        'niche': row['niche'],\n",
    "        'revenue': row['revenue'],\n",
    "        'traffic_latest': get_same_month_latest(traffic_top250, website, month),\n",
    "        'traffic_avg': get_same_month_avg(traffic_top250, website, month),\n",
    "        'traffic_lag30': get_lagged_traffic(traffic_top250, website, month, 30),\n",
    "        'traffic_lag60': get_lagged_traffic(traffic_top250, website, month, 60)\n",
    "    }\n",
    "    combined_records.append(record)\n",
    "\n",
    "combined_df = pd.DataFrame(combined_records)\n",
    "print(f\"\\nCombined dataset: {len(combined_df)} records\")\n",
    "combined_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data completeness\n",
    "print(\"Data completeness:\")\n",
    "for col in ['traffic_latest', 'traffic_avg', 'traffic_lag30', 'traffic_lag60']:\n",
    "    valid = combined_df[col].notna().sum()\n",
    "    pct = 100 * valid / len(combined_df)\n",
    "    print(f\"  {col}: {valid} valid ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation Analysis by Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_sites(n):\n",
    "    \"\"\"Get top N sites from the ranking\"\"\"\n",
    "    return [s.lower() for s in TOP_250_SITES[:n]]\n",
    "\n",
    "def calc_correlation(df, traffic_col, revenue_col='revenue'):\n",
    "    \"\"\"Calculate Pearson and Spearman correlations with p-values\"\"\"\n",
    "    # Drop rows with NaN in either column\n",
    "    valid = df[[traffic_col, revenue_col]].dropna()\n",
    "    \n",
    "    if len(valid) < 3:\n",
    "        return {'pearson_r': np.nan, 'pearson_p': np.nan, \n",
    "                'spearman_r': np.nan, 'spearman_p': np.nan, 'n': len(valid)}\n",
    "    \n",
    "    pearson_r, pearson_p = pearsonr(valid[traffic_col], valid[revenue_col])\n",
    "    spearman_r, spearman_p = spearmanr(valid[traffic_col], valid[revenue_col])\n",
    "    \n",
    "    return {\n",
    "        'pearson_r': pearson_r,\n",
    "        'pearson_p': pearson_p,\n",
    "        'spearman_r': spearman_r,\n",
    "        'spearman_p': spearman_p,\n",
    "        'n': len(valid)\n",
    "    }\n",
    "\n",
    "# Calculate correlations for each segment and method\n",
    "segments = [10, 50, 100, 250]\n",
    "traffic_methods = ['traffic_latest', 'traffic_avg', 'traffic_lag30', 'traffic_lag60']\n",
    "method_labels = ['Same-Month Latest', 'Same-Month Avg', '30-Day Lagged', '60-Day Lagged']\n",
    "\n",
    "results = []\n",
    "\n",
    "for seg_size in segments:\n",
    "    seg_sites = get_segment_sites(seg_size)\n",
    "    seg_data = combined_df[combined_df['website'].isin(seg_sites)]\n",
    "    \n",
    "    for method, label in zip(traffic_methods, method_labels):\n",
    "        corr = calc_correlation(seg_data, method)\n",
    "        results.append({\n",
    "            'Segment': f'Top {seg_size}',\n",
    "            'Method': label,\n",
    "            'Pearson r': corr['pearson_r'],\n",
    "            'Pearson p': corr['pearson_p'],\n",
    "            'Spearman r': corr['spearman_r'],\n",
    "            'Spearman p': corr['spearman_p'],\n",
    "            'N': corr['n']\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Correlation Results by Segment and Method:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table for easier comparison\n",
    "pivot_pearson = results_df.pivot(index='Segment', columns='Method', values='Pearson r')\n",
    "pivot_pearson = pivot_pearson[method_labels]  # Reorder columns\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PEARSON CORRELATION (r) - Traffic vs Revenue\")\n",
    "print(\"=\"*70)\n",
    "print(pivot_pearson.round(4).to_string())\n",
    "print(\"\\nHigher |r| = stronger correlation. Values closer to 1 or -1 are better.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pearson heatmap\n",
    "pivot_pearson_plot = results_df.pivot(index='Segment', columns='Method', values='Pearson r')\n",
    "pivot_pearson_plot = pivot_pearson_plot[method_labels]\n",
    "# Reorder rows\n",
    "pivot_pearson_plot = pivot_pearson_plot.reindex(['Top 10', 'Top 50', 'Top 100', 'Top 250'])\n",
    "\n",
    "sns.heatmap(pivot_pearson_plot, annot=True, fmt='.3f', cmap='RdYlGn', center=0, \n",
    "            ax=axes[0], vmin=-0.5, vmax=0.5)\n",
    "axes[0].set_title('Pearson Correlation (r)\\nTraffic vs Revenue', fontsize=12)\n",
    "axes[0].set_xlabel('')\n",
    "\n",
    "# Spearman heatmap\n",
    "pivot_spearman_plot = results_df.pivot(index='Segment', columns='Method', values='Spearman r')\n",
    "pivot_spearman_plot = pivot_spearman_plot[method_labels]\n",
    "pivot_spearman_plot = pivot_spearman_plot.reindex(['Top 10', 'Top 50', 'Top 100', 'Top 250'])\n",
    "\n",
    "sns.heatmap(pivot_spearman_plot, annot=True, fmt='.3f', cmap='RdYlGn', center=0,\n",
    "            ax=axes[1], vmin=-0.5, vmax=0.5)\n",
    "axes[1].set_title('Spearman Correlation (œÅ)\\nTraffic vs Revenue', fontsize=12)\n",
    "axes[1].set_xlabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Saved: correlation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best performing method for each segment\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST TRAFFIC METHOD BY SEGMENT (highest |Pearson r|)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for seg in ['Top 10', 'Top 50', 'Top 100', 'Top 250']:\n",
    "    seg_results = results_df[results_df['Segment'] == seg]\n",
    "    best_idx = seg_results['Pearson r'].abs().idxmax()\n",
    "    best = seg_results.loc[best_idx]\n",
    "    print(f\"\\n{seg}:\")\n",
    "    print(f\"  Best method: {best['Method']}\")\n",
    "    print(f\"  Pearson r = {best['Pearson r']:.4f} (p = {best['Pearson p']:.4f})\")\n",
    "    print(f\"  Spearman r = {best['Spearman r']:.4f}\")\n",
    "    print(f\"  Sample size: {best['N']:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Revenue Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Revenue Per Traffic Unit (RPTU) for each site\n",
    "# Using the best performing traffic method from above, or traffic_avg as default\n",
    "\n",
    "# Aggregate by site\n",
    "site_summary = combined_df.groupby('website').agg({\n",
    "    'revenue': 'sum',\n",
    "    'traffic_avg': 'mean',\n",
    "    'traffic_latest': 'mean',\n",
    "    'niche': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate RPTU (Revenue per Traffic Unit)\n",
    "site_summary['rptu'] = site_summary['revenue'] / site_summary['traffic_avg']\n",
    "site_summary['rptu'] = site_summary['rptu'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Add ranking\n",
    "site_summary['rank'] = site_summary['website'].apply(\n",
    "    lambda x: TOP_250_LOWER.index(x) + 1 if x in TOP_250_LOWER else 999\n",
    ")\n",
    "site_summary = site_summary.sort_values('rank')\n",
    "\n",
    "print(f\"Sites with RPTU calculated: {site_summary['rptu'].notna().sum()}\")\n",
    "site_summary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 most efficient sites (highest RPTU)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 10 MOST EFFICIENT SITES (Highest Revenue per Traffic Unit)\")\n",
    "print(\"=\"*70)\n",
    "top_efficient = site_summary.nlargest(10, 'rptu')[['rank', 'website', 'revenue', 'traffic_avg', 'rptu', 'niche']]\n",
    "top_efficient['rptu'] = top_efficient['rptu'].round(2)\n",
    "top_efficient['revenue'] = top_efficient['revenue'].apply(lambda x: f\"${x:,.0f}\")\n",
    "top_efficient['traffic_avg'] = top_efficient['traffic_avg'].round(0)\n",
    "print(top_efficient.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom 10 least efficient sites (lowest RPTU) - potential monetization opportunities\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BOTTOM 10 LEAST EFFICIENT SITES (Monetization Opportunities)\")\n",
    "print(\"=\"*70)\n",
    "bottom_efficient = site_summary[site_summary['rptu'].notna()].nsmallest(10, 'rptu')[['rank', 'website', 'revenue', 'traffic_avg', 'rptu', 'niche']]\n",
    "bottom_efficient['rptu'] = bottom_efficient['rptu'].round(2)\n",
    "bottom_efficient['revenue'] = bottom_efficient['revenue'].apply(lambda x: f\"${x:,.0f}\")\n",
    "bottom_efficient['traffic_avg'] = bottom_efficient['traffic_avg'].round(0)\n",
    "print(bottom_efficient.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Traffic vs Revenue with efficiency outliers highlighted\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "valid_sites = site_summary[site_summary['traffic_avg'].notna() & site_summary['revenue'].notna()]\n",
    "\n",
    "# Calculate efficiency quartiles\n",
    "q1 = valid_sites['rptu'].quantile(0.25)\n",
    "q3 = valid_sites['rptu'].quantile(0.75)\n",
    "\n",
    "# Color by efficiency\n",
    "colors = []\n",
    "for _, row in valid_sites.iterrows():\n",
    "    if pd.isna(row['rptu']):\n",
    "        colors.append('gray')\n",
    "    elif row['rptu'] >= q3:\n",
    "        colors.append('green')  # High efficiency\n",
    "    elif row['rptu'] <= q1:\n",
    "        colors.append('red')    # Low efficiency\n",
    "    else:\n",
    "        colors.append('blue')   # Average\n",
    "\n",
    "scatter = ax.scatter(valid_sites['traffic_avg'], valid_sites['revenue'], \n",
    "                     c=colors, alpha=0.6, s=50)\n",
    "\n",
    "ax.set_xlabel('Average Traffic (Monthly Estimate)', fontsize=12)\n",
    "ax.set_ylabel('Total Revenue ($)', fontsize=12)\n",
    "ax.set_title('Traffic vs Revenue by Site\\nGreen=High Efficiency, Red=Low Efficiency (Opportunities)', fontsize=14)\n",
    "\n",
    "# Add labels for top outliers\n",
    "for _, row in valid_sites.nlargest(5, 'rptu').iterrows():\n",
    "    ax.annotate(row['website'], (row['traffic_avg'], row['revenue']), fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('traffic_revenue_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Saved: traffic_revenue_scatter.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Additional Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Niche Performance Analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORRELATION BY NICHE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "niche_results = []\n",
    "for niche in combined_df['niche'].dropna().unique():\n",
    "    if pd.isna(niche) or niche == 'Unknown':\n",
    "        continue\n",
    "    niche_data = combined_df[combined_df['niche'] == niche]\n",
    "    if len(niche_data) < 10:\n",
    "        continue\n",
    "    \n",
    "    corr = calc_correlation(niche_data, 'traffic_avg')\n",
    "    niche_results.append({\n",
    "        'Niche': niche[:40],  # Truncate long names\n",
    "        'Sites': niche_data['website'].nunique(),\n",
    "        'Records': corr['n'],\n",
    "        'Pearson r': corr['pearson_r'],\n",
    "        'Spearman r': corr['spearman_r']\n",
    "    })\n",
    "\n",
    "niche_df = pd.DataFrame(niche_results).sort_values('Pearson r', ascending=False)\n",
    "print(niche_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio Concentration Analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PORTFOLIO CONCENTRATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Total revenue by site\n",
    "total_revenue = site_summary['revenue'].sum()\n",
    "site_summary_sorted = site_summary.sort_values('revenue', ascending=False)\n",
    "\n",
    "for n in [10, 50, 100]:\n",
    "    top_n_revenue = site_summary_sorted.head(n)['revenue'].sum()\n",
    "    pct = 100 * top_n_revenue / total_revenue\n",
    "    print(f\"Top {n} sites: ${top_n_revenue:,.0f} ({pct:.1f}% of total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonality Analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SEASONALITY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "combined_df['month_num'] = combined_df['month'].dt.month\n",
    "combined_df['month_name'] = combined_df['month'].dt.strftime('%b')\n",
    "\n",
    "monthly_avg = combined_df.groupby('month_num').agg({\n",
    "    'revenue': 'mean',\n",
    "    'traffic_avg': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "monthly_avg.index = [month_names[i-1] for i in monthly_avg.index]\n",
    "\n",
    "print(\"\\nAverage Revenue & Traffic by Month:\")\n",
    "print(monthly_avg.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonality visualization\n",
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "x = range(len(monthly_avg))\n",
    "ax1.bar(x, monthly_avg['revenue'], alpha=0.7, color='green', label='Avg Revenue')\n",
    "ax1.set_xlabel('Month')\n",
    "ax1.set_ylabel('Average Revenue ($)', color='green')\n",
    "ax1.tick_params(axis='y', labelcolor='green')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(monthly_avg.index)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(x, monthly_avg['traffic_avg'], color='orange', marker='o', linewidth=2, label='Avg Traffic')\n",
    "ax2.set_ylabel('Average Traffic', color='orange')\n",
    "ax2.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "plt.title('Seasonality: Average Revenue & Traffic by Month', fontsize=14)\n",
    "fig.legend(loc='upper right', bbox_to_anchor=(0.9, 0.9))\n",
    "plt.tight_layout()\n",
    "plt.savefig('seasonality.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Saved: seasonality.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä DATA OVERVIEW:\")\n",
    "print(f\"   ‚Ä¢ Sites analyzed: {combined_df['website'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Date range: {combined_df['month'].min().strftime('%b %Y')} to {combined_df['month'].max().strftime('%b %Y')}\")\n",
    "print(f\"   ‚Ä¢ Total revenue records: {len(combined_df)}\")\n",
    "\n",
    "print(\"\\nüìà CORRELATION FINDINGS:\")\n",
    "# Find overall best method\n",
    "best_overall = results_df.loc[results_df['Pearson r'].abs().idxmax()]\n",
    "print(f\"   ‚Ä¢ Strongest correlation found: {best_overall['Segment']} using {best_overall['Method']}\")\n",
    "print(f\"   ‚Ä¢ Pearson r = {best_overall['Pearson r']:.4f}\")\n",
    "\n",
    "# Check if lagged methods perform better\n",
    "lag30_avg = results_df[results_df['Method'] == '30-Day Lagged']['Pearson r'].abs().mean()\n",
    "lag60_avg = results_df[results_df['Method'] == '60-Day Lagged']['Pearson r'].abs().mean()\n",
    "same_month_avg = results_df[results_df['Method'] == 'Same-Month Latest']['Pearson r'].abs().mean()\n",
    "\n",
    "if lag30_avg > same_month_avg or lag60_avg > same_month_avg:\n",
    "    print(f\"   ‚Ä¢ ‚úì CONFIRMED: Lagged traffic shows stronger correlation than same-month\")\n",
    "    print(f\"     (30-day avg: {lag30_avg:.4f}, 60-day avg: {lag60_avg:.4f} vs same-month: {same_month_avg:.4f})\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Same-month traffic shows similar or stronger correlation than lagged\")\n",
    "\n",
    "print(\"\\nüí∞ EFFICIENCY INSIGHTS:\")\n",
    "if len(top_efficient) > 0:\n",
    "    print(f\"   ‚Ä¢ Most efficient site: {top_efficient.iloc[0]['website']}\")\n",
    "if len(bottom_efficient) > 0:\n",
    "    print(f\"   ‚Ä¢ Biggest monetization opportunity: {bottom_efficient.iloc[0]['website']}\")\n",
    "\n",
    "print(\"\\nüìÅ FILES SAVED:\")\n",
    "print(\"   ‚Ä¢ correlation_heatmap.png\")\n",
    "print(\"   ‚Ä¢ traffic_revenue_scatter.png\")\n",
    "print(\"   ‚Ä¢ seasonality.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
